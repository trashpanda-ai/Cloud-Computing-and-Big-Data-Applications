{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "- ```Python 3.9.18``` (conda env)\n",
    "- ```pip freeze > requirements.txt```\n",
    "- ```conda env export > environment.yml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "#from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import window, sum as _sum\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"serial_number\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"failure\", IntegerType(), True),\n",
    "    StructField(\"vault_id\", IntegerType(), True),\n",
    "    StructField(\"s1_read_error_rate\", IntegerType(), True),\n",
    "    StructField(\"s2_throughput_performance\", IntegerType(), True),\n",
    "    StructField(\"s3_spin_up_time\", IntegerType(), True),\n",
    "    StructField(\"s4_start_stop_count\", IntegerType(), True),\n",
    "    StructField(\"s5_reallocated_sector_count\", IntegerType(), True),\n",
    "    StructField(\"s7_seek_error_rate\", IntegerType(), True),\n",
    "    StructField(\"s8_seek_time_performance\", IntegerType(), True),\n",
    "    StructField(\"s9_power_on_hours\", IntegerType(), True),\n",
    "    StructField(\"s10_spin_retry_count\", IntegerType(), True),\n",
    "    StructField(\"s12_power_cycle_count\", IntegerType(), True),\n",
    "    StructField(\"s173_wear_leveling_count\", IntegerType(), True),\n",
    "    StructField(\"s174_unexpected_power_loss_count\", IntegerType(), True),\n",
    "    StructField(\"s183_sata_downshift_count\", IntegerType(), True),\n",
    "    StructField(\"s187_reported_uncorrectable_errors\", IntegerType(), True),\n",
    "    StructField(\"s188_command_timeout\", IntegerType(), True),\n",
    "    StructField(\"s189_high_fly_writes\", IntegerType(), True),\n",
    "    StructField(\"s190_airflow_temperature_cel\", IntegerType(), True),\n",
    "    StructField(\"s191_g_sense_error_rate\", IntegerType(), True),\n",
    "    StructField(\"s192_power_off_retract_count\", IntegerType(), True),\n",
    "    StructField(\"s193_load_unload_cycle_count\", IntegerType(), True),\n",
    "    StructField(\"s194_temperature_celsius\", IntegerType(), True),\n",
    "    StructField(\"s195_hardware_ecc_recovered\", IntegerType(), True),\n",
    "    StructField(\"s196_reallocated_event_count\", IntegerType(), True),\n",
    "    StructField(\"s197_current_pending_sector\", IntegerType(), True),\n",
    "    StructField(\"s198_offline_uncorrectable\", IntegerType(), True),\n",
    "    StructField(\"s199_udma_crc_error_count\", IntegerType(), True),\n",
    "    StructField(\"s200_multi_zone_error_rate\", IntegerType(), True),\n",
    "    StructField(\"s220_disk_shift\", IntegerType(), True),\n",
    "    StructField(\"s222_loaded_hours\", IntegerType(), True),\n",
    "    StructField(\"s223_load_retry_count\", IntegerType(), True),\n",
    "    StructField(\"s226_load_in_time\", IntegerType(), True),\n",
    "    StructField(\"s240_head_flying_hours\", IntegerType(), True),\n",
    "    StructField(\"s241_total_lbas_written\", IntegerType(), True),\n",
    "    StructField(\"s242_total_lbas_read\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingFromCSV\").getOrCreate()\n",
    "\n",
    "\n",
    "#read from csv file\n",
    "#drivedata = spark.readStream.csv(\"streaming_data_dir/drivedata-small.csv\", schema=schema, header=True)\n",
    "\n",
    "# Read the CSV files as a data stream\n",
    "streamingData = spark.readStream.schema(schema).option(\"maxFilesPerTrigger\", 1).csv(\"streaming_data_dir\")\n",
    "# print true if both are true\n",
    "print((streamingData.schema == schema)==( streamingData.isStreaming == True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply a window function to the streaming data\n",
    "windowedData = streamingData \\\n",
    "    .withWatermark(\"date\", \"31 days\") \\\n",
    "    .groupBy(\n",
    "        streamingData.vault_id,\n",
    "        window(streamingData.date, \"30 days\", \"1 day\"),\n",
    "        streamingData.model\n",
    "    ) \\\n",
    "    .agg(_sum(\"failure\").alias(\"total_failures\"))\n",
    "\n",
    "# Write the windowed data stream out to a memory sink for testing\n",
    "query = windowedData \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"windowedData\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
